# deepseek研究
## 本地大语言模型运行工具部署(Ollama)

**以下是 Ollama 的部署安装步骤：**

### Windows 系统
1. **下载安装包**：从 Ollama 官网下载适用于 Windows 的安装包（.exe 格式）。
2. **安装**：
    - **通过命令安装（推荐）**：以管理员身份运行 CMD，定位到 `OllamaSetup.exe` 所在的目录，执行以下命令（假设要安装到 `D:\Net_Program\Net_Ollama` 目录下）：
        ```bash
        OllamaSetup.exe /DIR="D:\Net_Program\Net_Ollama"
        ```
        然后点击弹出的安装窗体界面中的 `Install` 按钮，等待安装完成。
    - **通过鼠标双击安装（不推荐）**：直接双击安装包，点击 `Install` 按钮等待安装完成。
3. **验证安装**：在安装完成后，可以通过命令 `ollama --version` 验证是否安装成功。

### Linux 系统
1. **脚本安装**：
    - 打开终端，执行以下命令下载安装脚本：
        ```bash
        curl -fsSL https://ollama.com/install.sh | sh
        ```
    - 等待安装完成，安装脚本会自动下载所需的组件，并完成 Ollama 的安装与配置。
    - 安装完成后，可以通过以下命令启动 Ollama：
        ```bash
        ollama serve
        ```
2. **二进制安装**：
    - 将 Ollama 的二进制文件下载到 PATH 中的目录：
        ```bash
        sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
        sudo chmod +x /usr/bin/ollama
        ```
    - 将 Ollama 添加为自启动服务，为 Ollama 创建用户并创建服务文件：
        ```bash
        sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
        ```
        在 `/etc/systemd/system/ollama.service` 创建服务文件，内容如下：
        ```bash
        [Unit]
        Description=Ollama Service
        After=network-online.target

        [Service]
        ExecStart=/usr/bin/ollama serve
        User=ollama
        Group=ollama
        Restart=always
        RestartSec=3

        [Install]
        WantedBy=default.target
        ```
    - 设置开机自启动并启动 Ollama：
        ```bash
        sudo systemctl daemon-reload
        sudo systemctl enable ollama
        sudo systemctl start ollama
        ```
3. **安装特定版本**：设置 `OLLAMA_VERSION` 字段，可以安装对应的版本。例如：
    ```bash
    curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.3.13 sh
    ```
4. **查看日志**：查看作为启动服务运行的 Ollama 的日志：
    ```bash
    journalctl -e -u ollama
    ```
5. **更新**：通过以下命令更新 Ollama：
    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```
    或者下载 Ollama 二进制文件：
    ```bash
    sudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama
    sudo chmod +x /usr/bin/ollama
    ```

### 注意事项
- **检查系统兼容性**：Ollama 支持 macOS、Windows 和 Linux 操作系统，确保您的操作系统版本与 Ollama 的要求相匹配。
- **满足硬件要求**：内存需求至少 4GB RAM，推荐 8GB 以上；存储空间至少 100GB 硬盘空间。
- **配置模型存储路径**（可选）：在下载模型前，可以通过修改系统环境变量 `OLLAMA_MODELS` 来指定模型存储路径。
- **安装 Docker 容器环境**（可选）：如果选择通过 Docker 进行部署，需要先安装 Docker Desktop。

## 人工智能模型部署（DeepSeek）

**以下是通过 Ollama 部署 DeepSeek 模型的详细步骤：**

### 1. 下载 DeepSeek 模型
- 打开 Ollama 官网的模型库页面，搜索并选择 `DeepSeek-R1` 模型。
- 根据你的硬件配置选择合适的模型版本（如 1.5b、7b、14b 等）。不同版本对硬件的要求如下：
  - **1.5b**：适合显存较小的设备（如 NVIDIA RTX 3060 12GB）。
  - **7b**：适合显存 12GB 以上的设备。
  - **14b**：适合显存 12GB 以上的设备。
- 复制对应的下载命令，例如：
  ```bash
  ollama pull deepseek-r1:7b
  ```
  如果想下载其他版本，只需将 `7b` 替换为对应的版本号。

### 2. 运行模型
- 下载完成后，运行以下命令启动 DeepSeek 模型：
  ```bash
  ollama run deepseek-r1:7b
  ```
  如果一切正常，你将进入交互式命令行界面，可以直接与模型对话。

### 3. 其他注意事项
- 如果下载速度较慢，可以将 `ollama run` 替换为 `ollama pull`，这样可以在下载中断后重新执行命令，实现“断点续传”。
- 如果需要离线下载，可以参考魔塔社区或 HuggingFace 的国内镜像站。

通过以上步骤，你可以在本地成功部署并使用 DeepSeek 模型。

## 人工智能桌面客户端应用部署

以下是关于人工智能桌面客户端应用 **Chatbox** 和 **AnythingLLM** 的部署指南：

---

### Chatbox 部署指南
Chatbox 是一个用户友好的桌面客户端应用程序，支持多种大型语言模型（LLM），如 OpenAI、Ollama 等，并提供 Windows、MacOS、Linux、iOS 和 Android 版本。

#### 安装步骤：
1. **下载安装包**：
   - 访问 [Chatbox 官网](https://chatboxai.app/zh)，选择适合您操作系统的安装包进行下载。
   - 官网提供官方版和社区版，官方版更易于安装且功能更新。

2. **安装**：
   - 下载完成后，双击安装包并按照提示完成安装。

3. **配置模型**：
   - 打开 Chatbox 应用程序。
   - 在设置中选择 **Ollama API**，并配置已安装的模型（如 DeepSeek-R1）。

4. **使用**：
   - 配置完成后，即可在 Chatbox 中与选定的模型进行交互。

#### 特点：
- **本地数据存储**：数据存储在本地，确保隐私。
- **多语言支持**：支持多种语言，包括中文。
- **多平台支持**：支持 Windows、MacOS、Linux、iOS 和 Android。

---

### AnythingLLM Desktop部署指南
AnythingLLM 是一个开源的全栈应用程序，支持多种 LLM 和向量数据库，允许用户构建私有的 AI 系统。

#### 安装步骤：
1. **下载安装包**：
   - 访问 [AnythingLLM 官网](https://anythingllm.com/desktop)，根据您的系统选择对应的安装包。

2. **安装**：
   - 下载完成后，双击安装包并按照提示完成安装。

3. **配置模型**：
   - **LLM 首选项**：选择 **Ollama** 作为 LLM 提供商，确保本地 Ollama 模型已正常运行。
   - **向量数据库**：选择默认的 **LanceDB**。
   - **嵌入偏好**：安装并配置嵌入工具（如 **nomic-embed-text**）。

4. **创建和使用工作区**：
   - 启动 AnythingLLM 后，创建新的工作区并上传文档（如 PDF、TXT 等）。
   - 点击 **Save and Embed**，将文档嵌入到工作区。
   - 设置文档为当前对话的背景文档，即可开始与模型进行交互。

#### 特点：
- **多模型支持**：支持多种 LLM 和嵌入器。
- **数据隐私**：模型和聊天记录仅存储在本地。
- **知识库构建**：支持将文档嵌入到工作区，构建个性化知识库。

### 注意事项
- 在部署 AnythingLLM 时，需要确保 Ollama 已正确安装并运行。
- 如果需要使用 DeepSeek 模型，可以通过 Ollama 下载并配置。
- 如果您希望使用官方 API，可以在 AnythingLLM 中配置对应的 API Key。

---

### AnythingLLM Docker部署指南
以下是使用 Docker 部署 AnythingLLM 的详细步骤：

### 1. 拉取 AnythingLLM 镜像
- 打开终端或命令提示符，运行以下命令以拉取 AnythingLLM 的 Docker 镜像：
  ```bash
  docker pull mintplexlabs/anythingllm
  ```
- 可使用命令`docker images`查看镜像是否已成功拉取。

### 2. 设置存储路径
- 设置环境变量`STORAGE_LOCATION`以指定存储位置。例如，在 Linux 或 Mac 系统中，可运行以下命令：
  ```bash
  export STORAGE_LOCATION=$HOME/anythingllm
  ```
- 创建存储目录和`.env`文件：
  ```bash
  mkdir -p $STORAGE_LOCATION
  touch "$STORAGE_LOCATION/.env"
  ```

### 3. 运行 Docker 命令
- 运行以下命令以启动 AnythingLLM 容器：
  ```bash
  docker run -d -p 3001:3001 --cap-add SYS_ADMIN -v ${STORAGE_LOCATION}:/app/server/storage -v ${STORAGE_LOCATION}/.env:/app/server/.env -e STORAGE_DIR="/app/server/storage" mintplexlabs/anythingllm
  ```

- 该命令将容器的 3001 端口映射到主机的同一端口，添加了`SYS_ADMIN`能力，并挂载了存储目录和`.env`文件。

### 4. 访问 AnythingLLM 应用
- 在浏览器中打开`http://localhost:3001`，即可访问 AnythingLLM 的 Web 界面。

### 5. 配置 AnythingLLM
- 在 AnythingLLM 的 Web 界面中，按照提示进行配置：
  - **选择大模型类型**：选择“Ollama”作为大模型提供商，并确保 Ollama 已正常运行。
  - **配置嵌入模型和向量数据库**：可选择默认的嵌入模型和向量数据库（如 LanceDB）。
  - **添加知识库**：可上传 PDF、TXT 等文档以构建知识库。
  - **成员管理**：如需在局域网内多用户访问，可进行相应的用户管理设置。

### 注意事项
- **确保 Ollama 正常运行**：在配置 AnythingLLM 之前，需确保 Ollama 已正确安装并运行。
- **检查 Docker 进程**：如无法访问 AnythingLLM，可使用命令`docker ps`检查 Docker 进程是否已启动。
- **配置文件**：`.env`文件可用于存储环境变量，如 API 密钥等。


通过以上步骤，您可以轻松部署 Chatbox 和 AnythingLLM，打造属于自己的 AI 桌面应用。


## AnythingLLM 提供了两种主要的部署方式：桌面版（Desktop）和云版（Cloud）
### 以下是它们的主要区别和特点：

### **1. 桌面版（Desktop）**
- **部署方式**：
  - 支持 Windows、MacOS 和 Linux 系统。
  - 提供一键安装程序，适合个人单机使用。
- **功能特点**：
  - 适合个人用户，功能较为基础。
  - 不支持多用户、嵌入式聊天小部件、密码保护、邀请新用户等高级功能。
- **隐私与数据**：
  - 数据完全本地化，不依赖外部 API，确保隐私。
- **适用场景**：
  - 适合个人用户和小型团队，主要用于本地知识管理。

### **2. 云版（Cloud）**
- **部署方式**：
  - 支持通过 Docker 部署。
  - 支持多种云平台（如 AWS、GCP 等），方便远程部署。
- **功能特点**：
  - 支持多用户实例和权限管理。
  - 提供嵌入式聊天小部件，可用于网站集成。
  - 支持工作区隔离，适合企业级应用。
  - 提供完整的开发者 API，便于自定义集成。
- **隐私与数据**：
  - 数据可以存储在本地或云端，具体取决于部署方式。
- **适用场景**：
  - 适合需要多用户管理、文档知识库管理和高效检索的企业级应用。

### **桌面版 vs 云版**
| 特性 | 桌面版 | 云版 |
|------|--------|------|
| **部署方式** | 本地安装（Windows、MacOS、Linux） | Docker 部署、云平台部署 |
| **功能支持** | 基础功能，适合个人使用 | 多用户支持、权限管理、嵌入式小部件 |
| **数据隐私** | 数据完全本地化 | 数据存储灵活（本地或云端） |
| **适用场景** | 个人知识管理、单机使用 | 企业级应用、多用户协作 |
| **定制化能力** | 有限 | 高（支持 API 集成） |

### **总结**
- 如果您需要一个简单易用、隐私保护强的个人知识管理工具，**桌面版**是更好的选择。
- 如果您需要支持多用户协作、企业级功能和灵活的部署方式，**云版**更适合。


### 以下是基于 Docker 部署 Dify 的详细步骤，适用于私有化部署场景：

---

### **1. 前置条件**
1. **安装 Docker**：
   - 根据您使用的操作系统（如 Windows、Linux、macOS），从 [Docker 官网](https://www.docker.com/products/docker-desktop/) 下载并安装 Docker。
   - 确保 Docker 服务已启动且运行正常，可以通过命令 `docker -v` 验证。

2. **安装 Git**：
   - 确保系统已安装 Git，用于克隆 Dify 的源代码。

3. **系统资源**：
   - 确保服务器至少有 4GB 内存，建议分配更多资源以保证运行。

---

### **2. 部署步骤**

#### **步骤 1：克隆 Dify 源代码**
```bash
git clone https://github.com/langgenius/dify.git
cd dify/docker
```
将 `dify/docker` 目录复制出来，方便后续操作：
```bash
cp -r dify/docker dify-docker
cd dify-docker
```


#### **步骤 2：配置环境变量**
复制环境变量配置文件：
```bash
cp .env.example .env
```
根据需要编辑 `.env` 文件，例如配置邮件服务（如 RESEND API）。

#### **步骤 3：拉取镜像**
运行以下命令拉取所需镜像：
```bash
docker-compose pull
```
如果拉取失败，可以手动逐个拉取所需镜像，例如：
```bash
docker pull langgenius/dify-api:0.15.3
docker pull langgenius/dify-web:0.15.3
docker pull postgres:15-alpine
docker pull redis:6-alpine
```


#### **步骤 4：启动服务**
根据您的 Docker Compose 版本，选择合适的命令启动服务：
```bash
# Docker Compose V2
docker compose up -d

# Docker Compose V1
docker-compose up -d
```


#### **步骤 5：访问 Dify**
启动完成后，通过浏览器访问 `http://<服务器IP>:<端口>`，例如 `http://127.0.0.1:8001`。

---

### **3. 配置 Dify**
1. **创建管理员账户**：
   - 访问 Dify 的设置页面，创建管理员账户。

2. **配置模型**：
   - 在 Dify 中配置模型供应商（如 Ollama），并指定基础 URL（如 `http://host.docker.internal:11434`）。

3. **HTTPS 配置（可选）**：
   - 如果需要启用 HTTPS，将 SSL 证书和私钥复制到 `/dify/docker/nginx/ssl` 目录，并编辑 `docker-compose-template.yaml` 文件。

---

### **4. 注意事项**
1. **镜像加速**：
   - 如果在国内网络环境下拉取镜像失败，可以配置 Docker 镜像加速地址。
   - 修改 `docker-compose.yaml` 文件，添加国内镜像源。

2. **版本选择**：
   - 建议使用稳定版本（如 0.15.3），避免使用测试版本（如 1.0.0）。

3. **资源限制**：
   - 确保服务器资源充足，避免因资源不足导致服务运行异常。

---

通过以上步骤，您可以在服务器上通过 Docker 部署 Dify，实现私有化部署并支持多人使用。

## 本地知识库部署

以下是使用 AnythingLLM 搭建本地知识库的详细步骤：

1. **配置 AnythingLLM**：
   - 打开 AnythingLLM 应用程序，点击 **Get Started**。
   - 在 **LLM 首选项** 中选择 **Ollama**，并确保模型地址指向本地运行的 DeepSeek 模型（如 `http://127.0.0.1:11434`）。
   - 在 **嵌入偏好** 中选择默认的嵌入工具（如 `nomic-embed-text`）。
   - 在 **向量数据库** 中选择默认的 `LanceDB`。

2. **创建工作区**：
   - 在 AnythingLLM 中，点击 **New Workspace**，输入工作区名称（如 `My Knowledge Base`）。
   - 点击 **Save** 完成创建工作区。

3. **上传文档**：
   - 在工作区界面，点击 **Upload** 按钮。
   - 选择需要上传的文档（支持 PDF、TXT、DOCX 等格式）。
   - 点击 **Save and Embed**，系统将自动对文档进行切分和向量化处理。

4. **设置背景文档**：
   - 上传完成后，点击文档旁边的图钉按钮，将该文档设置为当前对话的背景文档。

5. **与知识库对话**：
   - 在工作区的输入框中输入问题，模型将根据上传的文档内容提供答案。
   - 您可以查看模型的推理过程和引用的文档。

6. **多工作区管理**：
   - 您可以创建多个工作区，每个工作区独立隔离。
   - 在不同工作区中上传不同的文档，实现知识库的分类管理。

7. **调整聊天模式**：
   - 在工作区设置中，调整 **聊天提示** 和 **查询模式拒绝响应** 的内容，以优化模型的回答。

通过以上步骤，您可以轻松搭建一个本地化的知识库，结合 DeepSeek 模型和 AnythingLLM 的强大功能，实现高效的文档管理和问答体验。

## 模型训练及微调

---

### **Unsloth部署**

#### 1. 下载 Miniconda 安装脚本
```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
```
- **功能**：从 Anaconda 官方网站下载 Miniconda 的安装脚本。


#### 2. 赋予安装脚本可执行权限
```bash
chmod +x Miniconda3-latest-Linux-x86_64.sh
```
- **功能**：使下载的安装脚本具有可执行权限。

#### 3. 运行 Miniconda 安装脚本
```bash
./Miniconda3-latest-Linux-x86_64.sh
```
- **功能**：运行安装脚本，安装 Miniconda。
- **提示**：在安装过程中，您需要接受许可协议、选择安装路径等。

#### 4. 配置环境变量
```bash
cat .bashrc
source .bashrc
```
- **功能**：查看 `.bashrc` 文件内容，并重新加载该文件，以确保 Miniconda 的路径被添加到环境变量中。
- **注意**：如果 Miniconda 安装脚本提示您是否将 `conda` 初始化到 `.bashrc`，建议选择“是”。如果未初始化，可以手动添加以下内容到 `.bashrc` 文件：
  ```bash
  export PATH="$HOME/miniconda3/bin:$PATH"
  ```

#### 5. 验证 Conda 安装
```bash
conda --version
```
- **功能**：检查 Conda 是否安装成功，并显示版本信息。

#### 6. 创建 Python 环境
```bash
conda create --name myenv python=3.11 -y
```
- **功能**：创建一个名为 `myenv` 的 Python 环境，并指定 Python 版本为 3.11。

#### 7. 激活环境
```bash
conda activate myenv
```
- **功能**：激活创建的 Python 环境。

#### 8. 安装常用库
```bash
conda install numpy pandas -y
```
- **功能**：在当前激活的环境中安装 NumPy 和 Pandas。


### **9. 检查环境配置**

#### **9.1 检查 `bitsandbytes`**
```bash
python -m bitsandbytes
```
- **功能**：运行 `bitsandbytes` 模块，检查其是否正确安装并运行。
- **用途**：`bitsandbytes` 是一个用于高效量化操作的库，Unsloth 依赖它来实现 4-bit 和 8-bit 的量化操作。

#### **9.2 检查 PyTorch 环境**
```bash
python -m torch.utils.collect_env
```
- **功能**：运行 PyTorch 的环境收集工具，打印出 PyTorch 的版本、CUDA 版本、GPU 驱动等信息。
- **用途**：确保 PyTorch 和 CUDA 环境正确配置，帮助排查潜在问题。

#### **9.3 检查 `xformers`**
```bash
python -m xformers.info
```
- **功能**：运行 `xformers` 的信息工具，检查其是否正确安装并运行。
- **用途**：`xformers` 是一个用于高效 Transformer 操作的库，Unsloth 依赖它来实现高效的模型训练和推理。

### **10. 创建 Conda 环境**

#### **10.1 创建环境**

```bash
conda create --name unsloth_env python=3.11 pytorch-cuda=12.1 pytorch torchvision torchaudio cudatoolkit=12.1 -c pytorch -c nvidia -y
```
- **功能**：创建一个名为 `unsloth_env` 的环境，安装 PyTorch 及其依赖，并指定 CUDA 版本为 12.1。
- **注意**：
  - 确保您的系统已安装 CUDA 12.1，否则需要调整 CUDA 版本以匹配您的 GPU 驱动。
  - 如果您不需要 GPU 支持，可以移除 `pytorch-cuda=12.1` 和 `cudatoolkit=12.1`。


```bash
conda create --name unsloth_env python=3.10 pytorch-cuda=11.8 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y
```
- **功能**：创建一个名为 `unsloth_env` 的 Conda 环境，安装 Python 3.10、PyTorch（支持 CUDA 11.8）、`cudatoolkit` 和 `xformers`。
- **参数说明**：
  - `python=3.10`：指定 Python 版本。
  - `pytorch-cuda=11.8`：指定 PyTorch 的 CUDA 版本。
  - `-c pytorch`、`-c nvidia`、`-c xformers`：指定 Conda 频道，确保从正确的源安装依赖。
  - `-y`：自动确认安装，无需手动输入。

#### **10.2 激活环境**
```bash
conda activate unsloth_env
```
- **功能**：激活创建的 `unsloth_env` 环境，确保后续命令在该环境中运行。

---

### **11. 安装 Unsloth**

#### **11.1 使用自动安装脚本**
```bash
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```
- **功能**：下载并运行 Unsloth 的自动安装脚本。
- **作用**：该脚本会检查当前环境中的 PyTorch 和 CUDA 版本，并生成适合的 `pip install` 命令，确保安装的 Unsloth 版本与您的环境兼容。

#### **11.2 手动安装 Unsloth**
```bash
pip install --upgrade pip
pip install "unsloth[cu118-torch250] @ git+https://github.com/unslothai/unsloth.git"
```
- **功能**：升级 `pip` 并安装 Unsloth。
- **参数说明**：
  - `--upgrade pip`：确保 `pip` 是最新版本。
  - `"unsloth[cu118-torch250] @ git+https://github.com/unslothai/unsloth.git"`：安装 Unsloth，指定 CUDA 版本和 PyTorch 版本。
    - `cu118` 表示 CUDA 11.8。
    - `torch250` 表示 PyTorch 2.5.0。

#### **11.3 简单安装 Unsloth**
```bash
pip install unsloth
```
- **功能**：直接安装 Unsloth，适用于已经满足依赖条件的环境。

---

### **12. 验证安装**
在完成安装后，可以通过以下命令验证 Unsloth 是否正确安装：
```bash
python -c "import unsloth; print(unsloth.__version__)"
```
- **功能**：运行 Python 命令，检查 Unsloth 的版本信息。

---

### **Unsloth微调模型**

```bash
# -*- coding: utf-8 -*-
"""
完整的 Unsloth 模型微调脚本
"""

import os
import torch
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

# 环境检查
def check_environment():
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is not available. Please ensure you have a compatible GPU.")
    print("CUDA is available. Proceeding with setup...")

# 加载预训练模型
def load_pretrained_model():
    print("Loading pre-trained model...")
    max_seq_length = 256  # 上下文长度
    load_in_4bit = True  # 使用 4 位量化
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="/home/deepseek/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit
    )
    return model, tokenizer

# 添加 LoRA 适配器
def add_lora_adapter(model):
    print("Adding LoRA adapter to the model...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # LoRA 的秩
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=16,  # 缩放因子
        lora_dropout=0,  # Dropout 概率
        bias="none",  # 偏置设置
        use_gradient_checkpointing="unsloth",  # 使用梯度检查点
        random_state=3407  # 随机种子
    )
    return model

# 准备数据集
def prepare_dataset():
    print("Preparing dataset...")
    # 示例：加载 Hugging Face 数据集
    # 替换为你的数据集名称和字段名
    dataset = load_dataset("/home/deepseek/test/mydata", split="train")
    #dataset = dataset.map(lambda x: {"text": x["your_text_field"]})
    
    # 只保留 input 和 output
    #dataset = dataset.map(lambda x: {"text": f"{x['input']}\n{x['output']}"})
    return dataset

# 配置训练参数
def configure_trainer(model, tokenizer, dataset):
    print("Configuring trainer...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=256,
        dataset_num_proc=8,
        args=TrainingArguments(
            per_device_train_batch_size=4,
            gradient_accumulation_steps=8,
            warmup_steps=5,
            max_steps=60,
            learning_rate=2e-4,
            fp16=False,  # 如果你的硬件支持，可以使用 fp16
            logging_steps=10,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs"
        )
    )
    return trainer

# 微调模型
def fine_tune(trainer):
    print("Starting fine-tuning...")
    trainer.train()
    print("Fine-tuning completed.")

# 保存模型
def save_model(model, tokenizer):
    print("Saving the fine-tuned model...")
    model.save_pretrained("fine_tuned_model", tokenizer, save_method="merged_16bit")
    print("Model saved successfully.")

# 测试模型
def test_model(model, tokenizer):
    print("Testing the fine-tuned model...")
    inputs = tokenizer(["你的测试提示"], return_tensors="pt").to("cuda")
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=1200
    )
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    print("Model response:", response[0])

# 主函数
def main():
    # 检查环境
    check_environment()
    
    # 加载预训练模型
    model, tokenizer = load_pretrained_model()
    
    # 添加 LoRA 适配器
    model = add_lora_adapter(model)
    
    # 准备数据集
    dataset = prepare_dataset()
    
    # 配置训练器
    trainer = configure_trainer(model, tokenizer, dataset)
    
    # 微调模型
    fine_tune(trainer)
    
    # 保存模型
    save_model(model, tokenizer)
    
    # 测试模型
    test_model(model, tokenizer)

if __name__ == "__main__":
    main()

```

### Llama-Factory 部署

```bash
    conda create -n llama_factory python=3.10
    conda activate llama_factory
    pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
    git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
    cd LLaMA-Factory
    pip install -e ".[torch,metrics]"
    conda activate llama_factory
    export USE_MODELSCOPE_HUB=1
    export CUDA_VISIBLE_DEVICES=0
    llamafactory-cli webui
```

以下是基于 LLaMA-Factory 的环境搭建和启动 Web UI 的详细步骤：

---

### 1. 创建 Conda 环境
```bash
conda create -n llama_factory python=3.10
```
**解释**：创建一个名为 `llama_factory` 的 Conda 环境，并指定 Python 版本为 3.10。这一步是为了隔离依赖，确保后续安装的包不会与系统中其他 Python 环境冲突。

---

### 2. 激活环境
```bash
conda activate llama_factory
```
**解释**：激活刚才创建的 `llama_factory` 环境。只有在激活该环境后，后续安装的包才会被正确安装到该环境中。

---

### 3. 安装 PyTorch 和相关依赖
```bash
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
```
**解释**：
- 安装 PyTorch（`torch`）、TorchVision（`torchvision`）和 TorchAudio（`torchaudio`）的特定版本（2.4.0），这些是深度学习任务中常用的库。
- `--index-url` 参数指定了 PyTorch 的官方 CUDA 12.1 镜像源（`cu121`），确保安装的是支持 CUDA 的版本。如果你的 GPU 使用的是其他版本的 CUDA（如 CUDA 11.6 或 CUDA 12.2），需要更换为对应的镜像源（如 `cu113` 或 `cu122`）。

---

### 4. 克隆 LLaMA-Factory 代码库
```bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
```
**解释**：
- 使用 Git 克隆 LLaMA-Factory 的代码库。`--depth 1` 参数表示只克隆最新的提交，而不是整个历史记录，这可以加快克隆速度。

---

### 5. 进入代码目录
```bash
cd LLaMA-Factory
```
**解释**：进入 LLaMA-Factory 的代码目录，后续的安装和运行命令需要在这个目录下执行。

---

### 6. 安装 LLaMA-Factory 的依赖
```bash
pip install -e ".[torch,metrics]"
```
**解释**：
- 使用 `pip install -e` 安装 LLaMA-Factory 的依赖。`-e` 参数表示以可编辑模式安装，这意味着你可以直接修改代码并立即看到效果。
- `.[torch,metrics]` 表示安装与 PyTorch 和评估指标相关的额外依赖。

---

### 7. 再次激活环境（可选）
```bash
conda activate llama_factory
```
**解释**：虽然前面已经激活了环境，但有时在切换目录或执行某些命令后，可能需要再次确认环境处于激活状态。

---

### 8. 设置使用 ModelScope Hub
```bash
export USE_MODELSCOPE_HUB=1
```
**解释**：设置环境变量 `USE_MODELSCOPE_HUB` 为 1，表示启用 ModelScope Hub。这允许你从 ModelScope Hub 下载预训练模型和数据集，作为 Hugging Face 的替代方案。

---

### 9. 指定 CUDA 设备
```bash
export CUDA_VISIBLE_DEVICES=0
```
**解释**：设置环境变量 `CUDA_VISIBLE_DEVICES` 为 0，表示只使用第一个 GPU 设备（设备编号从 0 开始）。如果你有多个 GPU，可以通过修改编号选择其他设备。这一步确保了 PyTorch 只使用指定的 GPU，避免资源冲突。

---

### 10. 启动 LLaMA-Factory Web UI
```bash
llamafactory-cli webui
```
**解释**：运行 `llamafactory-cli webui` 命令启动 LLaMA-Factory 的 Web UI。这将启动一个基于 Gradio 的 Web 界面，允许你通过浏览器进行模型的微调、推理等操作。

---

### 注意事项
1. **检查 CUDA 版本**：确保你的系统中安装的 CUDA 版本与 PyTorch 的版本兼容。如果 CUDA 版本不匹配，可能会导致 PyTorch 无法正常运行。
2. **网络问题**：如果在安装过程中遇到网络问题（如无法访问 PyTorch 或 ModelScope 的镜像源），可以尝试更换为国内的镜像源，例如：
   ```bash
   pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://mirror.tuna.tsinghua.edu.cn/pytorch/whl/cu121
   ```
3. **Web UI 访问**：启动 Web UI 后，通常会在终端中显示一个本地访问地址（如 `http://127.0.0.1:7860`）。通过浏览器访问该地址即可进入 Web 界面。

---

### 以下是使用 LLaMA-Factory 的 Web UI 进行模型微调的详细步骤：

### 1. 启动 Web UI
在终端中运行以下命令启动 LLaMA-Factory 的 Web UI：
```bash
llamafactory-cli webui
```
如果需要从 ModelScope 魔搭社区下载模型，可在命令前设置环境变量：
```bash
USE_MODELSCOPE_HUB=1 llamafactory-cli webui
```


启动后，浏览器会自动打开 Web UI 页面，通常地址为 `http://localhost:7860`。

### 2. 选择模型和微调方法
- 在 Web UI 中，选择要微调的模型，例如 `LLaMA3-8B-Chat`。
- 选择微调方法：
  - **full**：全参数微调，适合资源充足的情况。
  - **freeze**：冻结部分参数，适合资源有限的场景。
  - **lora**：LoRA 微调，高效且节省资源，推荐选择。

### 3. 配置训练参数
- **数据集**：选择或上传训练数据集。LLaMA-Factory 支持 Alpaca 和 ShareGPT 格式的数据集。
- **学习率**：建议设置为 `1e-4`。
- **计算类型**：根据显卡类型选择 `bf16` 或 `fp16`。
- **其他参数**：
  - 批量大小（`per_device_train_batch_size`）：建议设置为 2。
  - 梯度累计（`gradient_accumulation_steps`）：建议设置为 2。
  - 训练轮数（`num_train_epochs`）：根据需求设置，例如 10。

### 4. 开始微调
- 配置完成后，点击“开始”按钮开始微调。
- 训练过程中，Web UI 会显示进度、日志和损失曲线。

### 5. 模型评估
- 微调完成后，切换到 **Evaluate&Predict** 页签。
- 选择验证集（如 `eval`），并设置输出目录（如 `eval_llama3`）。
- 点击“开始”按钮进行评估，评估结果会保存到指定目录。

### 6. 模型推理
- 在 **Chat** 页签中，确保适配器路径指向微调后的模型（如 `train_llama3`）。
- 输入对话内容并提交，观察模型的生成结果。

### 注意事项
- 如果使用自定义数据集，需在 `data/data_info.json` 中添加数据集描述。
- 如果训练过程中报错，可尝试调整 `transformers` 版本。
- 微调完成后，建议重新加载模型以应用微调结果。

通过以上步骤，可以使用 LLaMA-Factory 的 Web UI 零代码微调大语言模型，并进行评估和推理。

###  命令行命令
   ```bash
 llamafactory-cli train     --stage pt     --do_train True     --model_name_or_path /home/deepseek/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B     --preprocessing_num_workers 1     --finetuning_type lora     --template deepseek3     --flash_attn auto     --dataset_dir /home/deepseek/test/data     --dataset my_dataset     --cutoff_len 256     --learning_rate 5e-05     --num_train_epochs 3.0     --max_samples 100000     --per_device_train_batch_size 2     --gradient_accumulation_steps 8     --lr_scheduler_type cosine     --max_grad_norm 1.0     --logging_steps 5     --save_steps 100     --warmup_steps 0     --packing True     --report_to none     --output_dir saves/DeepSeek-R1-1.5B-Distill/lora/train_2025-03-05-15-22-21     --bf16 True     --plot_loss True     --trust_remote_code True     --ddp_timeout 180000000     --include_num_input_tokens_seen True     --optim adamw_torch     --lora_rank 8     --lora_alpha 16     --lora_dropout 0     --lora_target all
   ```

## 模型准确性提升

### 提升大模型准确性的三种方法对比：提示词工程、知识库、微调

#### 1\. 提示词工程（Prompt Engineering）
提示词工程是指通过精心设计输入模型的提示（prompt），引导模型生成所需的输出。这种方法通过对提示词的优化，可以显著提高模型的准确性和响应质量，尤其在生成型任务中效果明显。

- **优点**：
  - 无需修改模型本身，快速实现效果提升。
  - 成本低，适合快速迭代。
  - 灵活性高，可根据不同任务调整提示词。

- **缺点**：
  - 需要丰富的领域知识和经验来设计有效的提示词。
  - 无法提升模型的深层次理解能力，复杂任务中表现可能不足。

- **适用场景**：
  - 文本生成、对话系统、信息检索等任务。

#### 2\. 知识库（Knowledge Base）
知识库是通过集成外部知识资源来丰富模型的知识面，从而提升模型在特定领域的准确性。通过将领域特定的知识、常识或实时数据嵌入到模型的输入或推理过程中，可以弥补模型的知识缺陷。

- **优点**：
  - 扩展模型知识面，提升特定领域的表现。
  - 助力模型对不常见问题的推理和回答。
  - 知识库可动态更新，确保模型获取最新信息。

- **缺点**：
  - 构建和维护知识库需要大量专家和人工干预。
  - 更新速度较慢，可能无法及时反映最新动态数据。
  - 不准确或不完整的知识库可能导致模型输出错误。

- **适用场景**：
  - 问答系统、虚拟助手、医疗诊断、法律咨询等领域。

#### 3\. 微调（Fine-tuning）
微调是在已有的大规模预训练模型基础上，通过标注数据进行训练，进一步优化模型的表现，以适应特定任务或场景的需求。微调通过直接修改模型参数来优化模型能力，能够显著提升模型在特定任务中的表现。

- **优点**：
  - 直接改善模型性能，尤其在特定任务或领域中。
  - 能深度定制模型，更好地适应应用场景。
  - 提升模型对特定领域的理解能力。

- **缺点**：
  - 需要高质量的标注数据，某些领域可能难以获取。
  - 需要较大的计算资源和时间，尤其是大模型。
  - 可能导致“灾难性遗忘”，影响模型在其他任务上的表现。

- **适用场景**：
  - 特定任务的性能提升，如分类、命名实体识别、文本生成等。

##### 3.1 全量微调与高效微调
- **全量微调**：使用完整数据对模型进行全面训练，适用于需要全面改进的场景，但计算资源消耗大。
- **高效微调**：只调整部分参数或通过特定机制（如 LoRA）进行微调，减少算力消耗，适合资源有限的环境。

##### 3.2 高效微调方法
- **LoRA（Low-Rank Adaptation）**：通过引入低秩矩阵减少微调参数数量，降低计算资源消耗。
- **QLoRA（Quantized LoRA）**：通过量化技术进一步优化 LoRA 的效率和计算成本。

### 4\. 高效微调的应用场景
- **对话风格微调**：调整模型的语气、礼貌程度等，提供个性化对话体验。
- **知识灌注**：将外部知识或领域特定信息注入模型，提升专业领域表现。
- **推理能力提升**：帮助模型更好地理解长文本中的隐含信息，提升逻辑推理能力。
- **Agent能力提升**：增强模型与其他系统的协作能力，提升任务调用策略。

### 5\. 微调与强化学习训练、模型蒸馏等概念辨析
- **微调**：基于预训练模型，使用少量数据进行再训练，优化特定任务表现。
- **强化学习**：通过奖励和惩罚机制训练模型，优化决策策略，适用于动态环境。
- **模型蒸馏**：将大模型的知识转移到小模型中，提升小模型的效率和性能。

### 6\. 三者对比总结
| 方法 | 优点 | 缺点 | 适用场景 |
| --- | --- | --- | --- |
| **提示词工程** | 快速、低成本、灵活 | 仅限于任务表层优化 | 文本生成、对话系统 |
| **知识库** | 扩展知识面，动态更新 | 构建和维护成本高 | 问答系统、专业领域应用 |
| **微调** | 深度定制，提升性能 | 需要大量数据和计算资源 | 分类、命名实体识别、专业领域 |

---

### 总结
- **提示词工程**：适合快速调整模型输出，无需修改模型结构。
- **知识库**：适用于需要丰富外部知识的场景，提升特定领域的准确性和可靠性。
- **微调**：适用于需要深度定制模型的场景，显著提升任务性能，但需要资源和时间。

根据任务需求，可以选择一种或多种方法结合使用，以达到最优效果。